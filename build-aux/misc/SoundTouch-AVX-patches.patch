From b7d59fc1632ab14cc9369b1ccabfeb871e8cc9a5 Mon Sep 17 00:00:00 2001
From: William Horvath <william@horvath.blog>
Date: Fri, 16 Jan 2026 04:19:33 -0800
Subject: [PATCH 1/2] Add an AVX TDStretch implementation.

Also fix UB in TDStretchSSE::calcCrossCorr, and take separate branches for aligned/unaligned loads.
---
 CMakeLists.txt                         |   1 +
 configure.ac                           |  30 +++
 include/FIFOSampleBuffer.h             |   2 +-
 include/STTypes.h                      |  26 ++-
 source/SoundTouch/FIFOSampleBuffer.cpp |   7 +-
 source/SoundTouch/Makefile.am          |  19 +-
 source/SoundTouch/TDStretch.cpp        |  53 +++--
 source/SoundTouch/TDStretch.h          |  18 +-
 source/SoundTouch/avx_optimized.cpp    | 304 +++++++++++++++++++++++++
 source/SoundTouch/cpu_detect.h         |   2 +
 source/SoundTouch/cpu_detect_x86.cpp   |  89 +++++---
 source/SoundTouch/sse_optimized.cpp    | 125 ++++++----
 source/SoundTouchDLL/Makefile.am       |   5 +-
 13 files changed, 574 insertions(+), 107 deletions(-)
 create mode 100644 source/SoundTouch/avx_optimized.cpp

diff --git a/CMakeLists.txt b/CMakeLists.txt
index 3047b93..d805237 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -35,6 +35,7 @@ add_library(SoundTouch
   source/SoundTouch/RateTransposer.cpp
   source/SoundTouch/SoundTouch.cpp
   source/SoundTouch/sse_optimized.cpp
+  source/SoundTouch/avx_optimized.cpp
   source/SoundTouch/TDStretch.cpp
 )
 target_include_directories(SoundTouch PUBLIC
diff --git a/configure.ac b/configure.ac
index d7c1efe..bd39f71 100644
--- a/configure.ac
+++ b/configure.ac
@@ -188,6 +188,35 @@ if test "x$enable_x86_optimizations" = "xyes" -a "x$ac_cv_header_cpuid_h" = "xye
 	# Disable optimizations in the SSTypes.h file if this is not the case.
 	if test "x$have_sse_intrinsics" = "xyes" ; then
 		echo "****** SSE support found ******"
+
+		# Skip checking for AVX in the case that SSE is not supported, since AVX
+		# is then guaranteed to also be unsupported.
+
+		# AVX support
+		original_saved_CXXFLAGS=$CXXFLAGS
+		have_avx_intrinsics=no
+		CXXFLAGS="-mavx -Winline $CXXFLAGS"
+
+		# Check if the user can compile AVX code using intrinsics.
+		# GCC supports AVX intrinsics since version 4.4
+		# A more recent GCC is recommended.
+		AC_COMPILE_IFELSE([AC_LANG_SOURCE([[
+		#if defined(__GNUC__) && (__GNUC__ < 4 || (__GNUC__ == 4 && __GNUC_MINOR__ < 4))
+		#error "Need GCC >= 4.4 for AVX intrinsics"
+		#endif
+		#include <immintrin.h>
+		int main () {
+			_mm256_setzero_ps();
+			return 0;
+		}]])],[have_avx_intrinsics=yes])
+		CXXFLAGS=$original_saved_CXXFLAGS
+
+		# Inform the user if we did or did not find AVX support.
+		if test "x$have_avx_intrinsics" = "xyes" ; then
+			echo "****** AVX support found ******"
+		else
+			echo "****** No AVX support found ******"
+		fi
 	else
 		echo "****** No SSE support found ******"
 		if test "x$enable_integer_samples" != "xyes"; then
@@ -265,6 +294,7 @@ AC_SUBST([AM_CXXFLAGS], [$AM_CXXFLAGS])
 # them if the user requested it.
 AM_CONDITIONAL([HAVE_MMX], [test "x$have_mmx_intrinsics" = "xyes"])
 AM_CONDITIONAL([HAVE_SSE], [test "x$have_sse_intrinsics" = "xyes"])
+AM_CONDITIONAL([HAVE_AVX], [test "x$have_avx_intrinsics" = "xyes"])
 
 
 dnl ############################################################################
diff --git a/include/FIFOSampleBuffer.h b/include/FIFOSampleBuffer.h
index e7cdbed..19f3cec 100644
--- a/include/FIFOSampleBuffer.h
+++ b/include/FIFOSampleBuffer.h
@@ -56,7 +56,7 @@ private:
     SAMPLETYPE *buffer;
 
     // Raw unaligned buffer memory. 'buffer' is made aligned by pointing it to first
-    // 16-byte aligned location of this buffer
+    // 16/32-byte aligned location of this buffer
     SAMPLETYPE *bufferUnaligned;
 
     /// Sample buffer size in bytes
diff --git a/include/STTypes.h b/include/STTypes.h
index 26e9af7..3e5e4a2 100644
--- a/include/STTypes.h
+++ b/include/STTypes.h
@@ -43,10 +43,6 @@ typedef unsigned long   ulong;
 #endif
 
 
-// Helper macro for aligning pointer up to next 16-byte boundary
-#define SOUNDTOUCH_ALIGN_POINTER_16(x)      ( ( (ulongptr)(x) + 15 ) & ~(ulongptr)15 )
-
-
 #if (defined(__GNUC__) && !defined(ANDROID))
     // In GCC, include soundtouch_config.h made by config scritps.
     // Skip this in Android compilation that uses GCC but without configure scripts.
@@ -157,6 +153,12 @@ namespace soundtouch
         #ifdef SOUNDTOUCH_ALLOW_X86_OPTIMIZATIONS
             // Allow SSE optimizations
             #define SOUNDTOUCH_ALLOW_SSE       1
+
+            #if !(defined(SOUNDTOUCH_ALLOW_AVX) && (SOUNDTOUCH_ALLOW_AVX + 0) == 0)
+                // Allow AVX optimizations, but also allow
+                // overriding AVX support with -DSOUNDTOUCH_ALLOW_AVX=0.
+                #define SOUNDTOUCH_ALLOW_AVX   1
+            #endif
         #endif
 
     #endif  // SOUNDTOUCH_INTEGER_SAMPLES
@@ -169,6 +171,22 @@ namespace soundtouch
 
 }
 
+#if !defined(SOUNDTOUCH_ALLOW_AVX) || (SOUNDTOUCH_ALLOW_AVX + 0) == 0
+#define SOUNDTOUCH_OPTIMAL_POINTER_ALIGNMENT 15
+#else
+// If AVX is desired, align to 32 bytes to be safe. This is a small tradeoff for
+// increased memory usage over checking cpuid bits more often than necessary
+#define SOUNDTOUCH_OPTIMAL_POINTER_ALIGNMENT 31
+#endif
+
+#define SOUNDTOUCH_OPTIMAL_DIVISIBILITY ((SOUNDTOUCH_OPTIMAL_POINTER_ALIGNMENT + 1) / 2)
+
+// Helper macro for aligning pointer up to next 16-byte boundary (for SSE/MMX-specific routines)
+#define SOUNDTOUCH_ALIGN_POINTER_16(x)      ( ( (ulongptr)(x) + 15 ) & ~(ulongptr)15 )
+
+// Helper macro for aligning pointer up to next optimal boundary
+#define SOUNDTOUCH_ALIGN_POINTER(x)      ( ( (ulongptr)(x) + SOUNDTOUCH_OPTIMAL_POINTER_ALIGNMENT ) & ~(ulongptr)SOUNDTOUCH_OPTIMAL_POINTER_ALIGNMENT )
+
 // define ST_NO_EXCEPTION_HANDLING switch to disable throwing std exceptions:
 // #define ST_NO_EXCEPTION_HANDLING    1
 #ifdef ST_NO_EXCEPTION_HANDLING
diff --git a/source/SoundTouch/FIFOSampleBuffer.cpp b/source/SoundTouch/FIFOSampleBuffer.cpp
index 9e0d5b2..0e77abb 100644
--- a/source/SoundTouch/FIFOSampleBuffer.cpp
+++ b/source/SoundTouch/FIFOSampleBuffer.cpp
@@ -165,13 +165,14 @@ void FIFOSampleBuffer::ensureCapacity(uint capacityRequirement)
         // enlarge the buffer in 4kbyte steps (round up to next 4k boundary)
         sizeInBytes = (capacityRequirement * channels * sizeof(SAMPLETYPE) + 4095) & (uint)-4096;
         assert(sizeInBytes % 2 == 0);
-        tempUnaligned = new SAMPLETYPE[sizeInBytes / sizeof(SAMPLETYPE) + 16 / sizeof(SAMPLETYPE)];
+        tempUnaligned = new SAMPLETYPE[sizeInBytes / sizeof(SAMPLETYPE) +
+                                       SOUNDTOUCH_OPTIMAL_POINTER_ALIGNMENT / sizeof(SAMPLETYPE)];
         if (tempUnaligned == nullptr)
         {
             ST_THROW_RT_ERROR("Couldn't allocate memory!\n");
         }
-        // Align the buffer to begin at 16byte cache line boundary for optimal performance
-        temp = (SAMPLETYPE *)SOUNDTOUCH_ALIGN_POINTER_16(tempUnaligned);
+        // Align the buffer to begin at 16/32byte cache line boundary for optimal performance
+        temp = (SAMPLETYPE *)SOUNDTOUCH_ALIGN_POINTER(tempUnaligned);
         if (samplesInBuffer)
         {
             memcpy(temp, ptrBegin(), samplesInBuffer * channels * sizeof(SAMPLETYPE));
diff --git a/source/SoundTouch/Makefile.am b/source/SoundTouch/Makefile.am
index e0548f9..4228f7a 100644
--- a/source/SoundTouch/Makefile.am
+++ b/source/SoundTouch/Makefile.am
@@ -35,11 +35,12 @@ libSoundTouch_la_SOURCES=AAFilter.cpp FIRFilter.cpp FIFOSampleBuffer.cpp    \
 # Compiler flags
 #AM_CXXFLAGS+=
 
-# Compile the files that need MMX and SSE individually.
-libSoundTouch_la_LIBADD=libSoundTouchMMX.la libSoundTouchSSE.la
-noinst_LTLIBRARIES=libSoundTouchMMX.la libSoundTouchSSE.la
+# Compile the files that need MMX, SSE, and AVX individually.
+libSoundTouch_la_LIBADD=libSoundTouchMMX.la libSoundTouchSSE.la libSoundTouchAVX.la
+noinst_LTLIBRARIES=libSoundTouchMMX.la libSoundTouchSSE.la libSoundTouchAVX.la
 libSoundTouchMMX_la_SOURCES=mmx_optimized.cpp
 libSoundTouchSSE_la_SOURCES=sse_optimized.cpp
+libSoundTouchAVX_la_SOURCES=avx_optimized.cpp
 
 # We enable optimizations by default.
 # If MMX is supported compile with -mmmx.
@@ -58,10 +59,18 @@ else
 libSoundTouchSSE_la_CXXFLAGS = $(AM_CXXFLAGS)
 endif
 
+# If AVX is supported compile with -mavx.
+if HAVE_AVX
+libSoundTouchAVX_la_CXXFLAGS = -mavx $(AM_CXXFLAGS)
+else
+libSoundTouchAVX_la_CXXFLAGS = $(AM_CXXFLAGS)
+endif
+
 # Let the user disable optimizations if he wishes to.
 if !X86_OPTIMIZATIONS
 libSoundTouchMMX_la_CXXFLAGS = $(AM_CXXFLAGS)
 libSoundTouchSSE_la_CXXFLAGS = $(AM_CXXFLAGS)
+libSoundTouchAVX_la_CXXFLAGS = $(AM_CXXFLAGS)
 endif
 
 # Modify the default 0.0.0 to LIB_SONAME.0.0
@@ -70,5 +79,5 @@ libSoundTouch_la_LDFLAGS=-version-info @LIB_SONAME@
 # other linking flags to add
 # noinst_LTLIBRARIES = libSoundTouchOpt.la
 # libSoundTouch_la_LIBADD = libSoundTouchOpt.la
-# libSoundTouchOpt_la_SOURCES = mmx_optimized.cpp sse_optimized.cpp
-# libSoundTouchOpt_la_CXXFLAGS = -O3 -msse -fcheck-new -I../../include
+# libSoundTouchOpt_la_SOURCES = mmx_optimized.cpp sse_optimized.cpp avx_optimized.cpp
+# libSoundTouchOpt_la_CXXFLAGS = -O3 -msse -mavx -fcheck-new -I../../include
diff --git a/source/SoundTouch/TDStretch.cpp b/source/SoundTouch/TDStretch.cpp
index 93ac181..76c47e0 100644
--- a/source/SoundTouch/TDStretch.cpp
+++ b/source/SoundTouch/TDStretch.cpp
@@ -660,11 +660,11 @@ void TDStretch::processSamples()
             // in SIMD mode, round the skip amount to value corresponding to aligned memory address
             if (channels == 1)
             {
-                skip &= -4;
+                skip &= -(SOUNDTOUCH_OPTIMAL_DIVISIBILITY / 2);
             }
             else if (channels == 2)
             {
-                skip &= -2;
+                skip &= -(SOUNDTOUCH_OPTIMAL_DIVISIBILITY / 4);
             }
             #endif
             skipFract -= skip;
@@ -729,9 +729,10 @@ void TDStretch::acceptNewOverlapLength(int newOverlapLength)
     {
         delete[] pMidBufferUnaligned;
 
-        pMidBufferUnaligned = new SAMPLETYPE[overlapLength * channels + 16 / sizeof(SAMPLETYPE)];
-        // ensure that 'pMidBuffer' is aligned to 16 byte boundary for efficiency
-        pMidBuffer = (SAMPLETYPE *)SOUNDTOUCH_ALIGN_POINTER_16(pMidBufferUnaligned);
+        pMidBufferUnaligned = new SAMPLETYPE[overlapLength * channels +
+                                             SOUNDTOUCH_OPTIMAL_POINTER_ALIGNMENT / sizeof(SAMPLETYPE)];
+        // ensure that 'pMidBuffer' is aligned to 16/32 byte boundary for efficiency
+        pMidBuffer = (SAMPLETYPE *)SOUNDTOUCH_ALIGN_POINTER(pMidBufferUnaligned);
 
         clearMidBuffer();
     }
@@ -755,17 +756,15 @@ TDStretch * TDStretch::newInstance()
     uExtensions = detectCPUextensions();
     (void)uExtensions;
 
-    // Check if MMX/SSE instruction set extensions supported by CPU
-
-#ifdef SOUNDTOUCH_ALLOW_MMX
-    // MMX routines available only with integer sample types
-    if (uExtensions & SUPPORT_MMX)
+    // Check if MMX/SSE/AVX instruction set extensions supported by CPU
+#if defined(SOUNDTOUCH_ALLOW_AVX) && (SOUNDTOUCH_ALLOW_AVX + 0) == 1
+    if (uExtensions & SUPPORT_AVX)
     {
-        return ::new TDStretchMMX;
+        // AVX support
+        return ::new TDStretchAVX(uExtensions & SUPPORT_FMA);
     }
     else
-#endif // SOUNDTOUCH_ALLOW_MMX
-
+#endif // defined(SOUNDTOUCH_ALLOW_AVX) && (SOUNDTOUCH_ALLOW_AVX + 0) == 1
 
 #ifdef SOUNDTOUCH_ALLOW_SSE
     if (uExtensions & SUPPORT_SSE)
@@ -776,6 +775,16 @@ TDStretch * TDStretch::newInstance()
     else
 #endif // SOUNDTOUCH_ALLOW_SSE
 
+#ifdef SOUNDTOUCH_ALLOW_MMX
+    // MMX routines available only with integer sample types
+    if (uExtensions & SUPPORT_MMX)
+    {
+        return ::new TDStretchMMX;
+    }
+    else
+#endif // SOUNDTOUCH_ALLOW_MMX
+
+
     {
         // ISA optimizations not supported, use plain C version
         return ::new TDStretch;
@@ -1014,10 +1023,10 @@ void TDStretch::calculateOverlapLength(int overlapInMsec)
 
     assert(overlapInMsec >= 0);
     newOvl = (sampleRate * overlapInMsec) / 1000;
-    if (newOvl < 16) newOvl = 16;
+    if (newOvl < SOUNDTOUCH_OPTIMAL_DIVISIBILITY * 2) newOvl = SOUNDTOUCH_OPTIMAL_DIVISIBILITY * 2;
 
-    // must be divisible by 8
-    newOvl -= newOvl % 8;
+    // must be divisible by 8/16
+    newOvl -= newOvl % SOUNDTOUCH_OPTIMAL_DIVISIBILITY;
 
     acceptNewOverlapLength(newOvl);
 }
@@ -1031,12 +1040,12 @@ double TDStretch::calcCrossCorr(const float *mixingPos, const float *compare, do
     int i;
 
     #ifdef ST_SIMD_AVOID_UNALIGNED
-        // in SIMD mode skip 'mixingPos' positions that aren't aligned to 16-byte boundary
-        if (((ulongptr)mixingPos) & 15) return -1e50;
+        // in SIMD mode skip 'mixingPos' positions that aren't aligned to 16/32-byte boundaries
+        if (((ulongptr)mixingPos) & SOUNDTOUCH_OPTIMAL_POINTER_ALIGNMENT) return -1e50;
     #endif
 
-    // hint compiler autovectorization that loop length is divisible by 8
-    int ilength = (channels * overlapLength) & -8;
+    // hint compiler autovectorization that loop length is divisible by 8/16
+    int ilength = (channels * overlapLength) & -SOUNDTOUCH_OPTIMAL_DIVISIBILITY;
 
     corr = norm = 0;
     // Same routine for stereo and mono
@@ -1065,8 +1074,8 @@ double TDStretch::calcCrossCorrAccumulate(const float *mixingPos, const float *c
         norm -= mixingPos[-i] * mixingPos[-i];
     }
 
-    // hint compiler autovectorization that loop length is divisible by 8
-    int ilength = (channels * overlapLength) & -8;
+    // hint compiler autovectorization that loop length is divisible by 8/16
+    int ilength = (channels * overlapLength) & -SOUNDTOUCH_OPTIMAL_DIVISIBILITY;
 
     // Same routine for stereo and mono
     for (i = 0; i < ilength; i ++)
diff --git a/source/SoundTouch/TDStretch.h b/source/SoundTouch/TDStretch.h
index 483dd53..5a07268 100644
--- a/source/SoundTouch/TDStretch.h
+++ b/source/SoundTouch/TDStretch.h
@@ -5,7 +5,7 @@
 /// with several performance-increasing tweaks.
 ///
 /// Note : MMX/SSE optimized functions reside in separate, platform-specific files
-/// 'mmx_optimized.cpp' and 'sse_optimized.cpp'
+/// 'mmx_optimized.cpp', 'sse_optimized.cpp', and 'avx_optimized.cpp'
 ///
 /// Author        : Copyright (c) Olli Parviainen
 /// Author e-mail : oparviai 'at' iki.fi
@@ -275,5 +275,21 @@ public:
 
 #endif /// SOUNDTOUCH_ALLOW_SSE
 
+
+#if defined(SOUNDTOUCH_ALLOW_AVX) && (SOUNDTOUCH_ALLOW_AVX + 0) == 1
+    /// Class that implements AVX optimized routines for floating point samples type.
+    class TDStretchAVX : public TDStretch
+    {
+    private:
+        bool bSupportsFMA;
+    protected:
+        double calcCrossCorr(const float *mixingPos, const float *compare, double &norm) override;
+        double calcCrossCorrAccumulate(const float *mixingPos, const float *compare, double &norm) override;
+    public:
+        TDStretchAVX(bool supportsFMA);
+    };
+
+#endif /// defined(SOUNDTOUCH_ALLOW_AVX) && (SOUNDTOUCH_ALLOW_AVX + 0) == 1
+
 }
 #endif  /// TDStretch_H
diff --git a/source/SoundTouch/avx_optimized.cpp b/source/SoundTouch/avx_optimized.cpp
new file mode 100644
index 0000000..d7496de
--- /dev/null
+++ b/source/SoundTouch/avx_optimized.cpp
@@ -0,0 +1,304 @@
+////////////////////////////////////////////////////////////////////////////////
+///
+/// AVX optimized routines for TDStretch algorithms. All AVX
+/// optimized functions have been gathered into this single source
+/// code file, regardless to their class or original source code file, in order
+/// to ease porting the library to other compiler and processor platforms.
+///
+/// The AVX-optimizations are programmed using AVX compiler intrinsics that
+/// are supported both by Microsoft Visual C++ and GCC compilers, so this file
+/// should compile with both toolsets.
+///
+/// Author        : Copyright (c) Olli Parviainen
+/// Author e-mail : oparviai 'at' iki.fi
+/// SoundTouch WWW: http://www.surina.net/soundtouch
+///
+////////////////////////////////////////////////////////////////////////////////
+//
+// License :
+//
+//  SoundTouch audio processing library
+//  Copyright (c) Olli Parviainen
+//  Copyright (c) William Horvath
+//
+//  This library is free software; you can redistribute it and/or
+//  modify it under the terms of the GNU Lesser General Public
+//  License as published by the Free Software Foundation; either
+//  version 2.1 of the License, or (at your option) any later version.
+//
+//  This library is distributed in the hope that it will be useful,
+//  but WITHOUT ANY WARRANTY; without even the implied warranty of
+//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+//  Lesser General Public License for more details.
+//
+//  You should have received a copy of the GNU Lesser General Public
+//  License along with this library; if not, write to the Free Software
+//  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+//
+////////////////////////////////////////////////////////////////////////////////
+
+#include "cpu_detect.h"
+#include "STTypes.h"
+
+using namespace soundtouch;
+
+#if defined(SOUNDTOUCH_ALLOW_AVX) && (SOUNDTOUCH_ALLOW_AVX + 0) == 1
+
+// AVX routines available only with float sample type
+
+//////////////////////////////////////////////////////////////////////////////
+//
+// implementation of AVX optimized functions of class 'TDStretchAVX'
+//
+//////////////////////////////////////////////////////////////////////////////
+
+#include "TDStretch.h"
+#include <immintrin.h>
+#include <math.h>
+
+TDStretchAVX::TDStretchAVX(bool supportsFMA) : TDStretch() {
+    bSupportsFMA = supportsFMA;
+}
+
+// Calculates cross correlation of two buffers
+double TDStretchAVX::calcCrossCorr(const float *pV1, const float *pV2, double &anorm)
+{
+    int i;
+    const float *pVec1;
+    const float *pVec2;
+    __m256 vSum, vNorm;
+
+#ifdef ST_SIMD_AVOID_UNALIGNED
+    // Skip unaligned locations, isAligned is always true after this point
+    if (((ulongptr)pV1) & 31) return -1e50;
+    #define IS_ALIGNED true
+#else
+    // Check alignment at runtime for optimal load instruction selection
+    const bool isAligned = (((ulongptr)pV1) & 31) == 0;
+    #define IS_ALIGNED isAligned
+#endif
+
+    // ensure overlapLength is divisible by 16
+    assert((overlapLength % 16) == 0);
+
+    pVec1 = pV1;
+    pVec2 = pV2;
+    vSum = _mm256_setzero_ps();
+    vNorm = _mm256_setzero_ps();
+
+    // Calculate total number of floats and how many full 32-float blocks we have
+    const int totalFloats = channels * overlapLength;
+    const int fullBlocks = totalFloats / 32;
+    const int remainderFloats = totalFloats - (fullBlocks * 32);
+
+    // TODO: somehow avoid duplication without sacrificing performance here...
+    if (bSupportsFMA)
+    {
+        // Process 32 floats per iteration (4 x 8-float AVX vectors)
+        if (IS_ALIGNED)
+        {
+            for (i = 0; i < fullBlocks; i++)
+            {
+                __m256 vTemp, vRef;
+
+                vTemp = _mm256_load_ps(pVec1);
+                vRef  = _mm256_load_ps(pVec2);
+                vSum  = _mm256_fmadd_ps(vTemp, vRef, vSum);
+                vNorm = _mm256_fmadd_ps(vTemp, vTemp, vNorm);
+
+                vTemp = _mm256_load_ps(pVec1 + 8);
+                vRef  = _mm256_load_ps(pVec2 + 8);
+                vSum  = _mm256_fmadd_ps(vTemp, vRef, vSum);
+                vNorm = _mm256_fmadd_ps(vTemp, vTemp, vNorm);
+
+                vTemp = _mm256_load_ps(pVec1 + 16);
+                vRef  = _mm256_load_ps(pVec2 + 16);
+                vSum  = _mm256_fmadd_ps(vTemp, vRef, vSum);
+                vNorm = _mm256_fmadd_ps(vTemp, vTemp, vNorm);
+
+                vTemp = _mm256_load_ps(pVec1 + 24);
+                vRef  = _mm256_load_ps(pVec2 + 24);
+                vSum  = _mm256_fmadd_ps(vTemp, vRef, vSum);
+                vNorm = _mm256_fmadd_ps(vTemp, vTemp, vNorm);
+
+                pVec1 += 32;
+                pVec2 += 32;
+            }
+        }
+        else
+        {
+            for (i = 0; i < fullBlocks; i++)
+            {
+                __m256 vTemp, vRef;
+
+                vTemp = _mm256_loadu_ps(pVec1);
+                vRef  = _mm256_load_ps(pVec2);
+                vSum  = _mm256_fmadd_ps(vTemp, vRef, vSum);
+                vNorm = _mm256_fmadd_ps(vTemp, vTemp, vNorm);
+
+                vTemp = _mm256_loadu_ps(pVec1 + 8);
+                vRef  = _mm256_load_ps(pVec2 + 8);
+                vSum  = _mm256_fmadd_ps(vTemp, vRef, vSum);
+                vNorm = _mm256_fmadd_ps(vTemp, vTemp, vNorm);
+
+                vTemp = _mm256_loadu_ps(pVec1 + 16);
+                vRef  = _mm256_load_ps(pVec2 + 16);
+                vSum  = _mm256_fmadd_ps(vTemp, vRef, vSum);
+                vNorm = _mm256_fmadd_ps(vTemp, vTemp, vNorm);
+
+                vTemp = _mm256_loadu_ps(pVec1 + 24);
+                vRef  = _mm256_load_ps(pVec2 + 24);
+                vSum  = _mm256_fmadd_ps(vTemp, vRef, vSum);
+                vNorm = _mm256_fmadd_ps(vTemp, vTemp, vNorm);
+
+                pVec1 += 32;
+                pVec2 += 32;
+            }
+        }
+    }
+    else
+    {
+        if (IS_ALIGNED)
+        {
+            for (i = 0; i < fullBlocks; i++)
+            {
+                __m256 vTemp, vRef;
+
+                vTemp = _mm256_load_ps(pVec1);
+                vRef  = _mm256_load_ps(pVec2);
+                vSum  = _mm256_add_ps(vSum, _mm256_mul_ps(vTemp, vRef));
+                vNorm = _mm256_add_ps(vNorm, _mm256_mul_ps(vTemp, vTemp));
+
+                vTemp = _mm256_load_ps(pVec1 + 8);
+                vRef  = _mm256_load_ps(pVec2 + 8);
+                vSum  = _mm256_add_ps(vSum, _mm256_mul_ps(vTemp, vRef));
+                vNorm = _mm256_add_ps(vNorm, _mm256_mul_ps(vTemp, vTemp));
+
+                vTemp = _mm256_load_ps(pVec1 + 16);
+                vRef  = _mm256_load_ps(pVec2 + 16);
+                vSum  = _mm256_add_ps(vSum, _mm256_mul_ps(vTemp, vRef));
+                vNorm = _mm256_add_ps(vNorm, _mm256_mul_ps(vTemp, vTemp));
+
+                vTemp = _mm256_load_ps(pVec1 + 24);
+                vRef  = _mm256_load_ps(pVec2 + 24);
+                vSum  = _mm256_add_ps(vSum, _mm256_mul_ps(vTemp, vRef));
+                vNorm = _mm256_add_ps(vNorm, _mm256_mul_ps(vTemp, vTemp));
+
+                pVec1 += 32;
+                pVec2 += 32;
+            }
+        }
+        else
+        {
+            for (i = 0; i < fullBlocks; i++)
+            {
+                __m256 vTemp, vRef;
+
+                vTemp = _mm256_loadu_ps(pVec1);
+                vRef  = _mm256_load_ps(pVec2);
+                vSum  = _mm256_add_ps(vSum, _mm256_mul_ps(vTemp, vRef));
+                vNorm = _mm256_add_ps(vNorm, _mm256_mul_ps(vTemp, vTemp));
+
+                vTemp = _mm256_loadu_ps(pVec1 + 8);
+                vRef  = _mm256_load_ps(pVec2 + 8);
+                vSum  = _mm256_add_ps(vSum, _mm256_mul_ps(vTemp, vRef));
+                vNorm = _mm256_add_ps(vNorm, _mm256_mul_ps(vTemp, vTemp));
+
+                vTemp = _mm256_loadu_ps(pVec1 + 16);
+                vRef  = _mm256_load_ps(pVec2 + 16);
+                vSum  = _mm256_add_ps(vSum, _mm256_mul_ps(vTemp, vRef));
+                vNorm = _mm256_add_ps(vNorm, _mm256_mul_ps(vTemp, vTemp));
+
+                vTemp = _mm256_loadu_ps(pVec1 + 24);
+                vRef  = _mm256_load_ps(pVec2 + 24);
+                vSum  = _mm256_add_ps(vSum, _mm256_mul_ps(vTemp, vRef));
+                vNorm = _mm256_add_ps(vNorm, _mm256_mul_ps(vTemp, vTemp));
+
+                pVec1 += 32;
+                pVec2 += 32;
+            }
+        }
+    }
+
+    // Reduce 256-bit to 128-bit by adding high and low halves
+    __m128 vNorm128 = _mm_add_ps(_mm256_castps256_ps128(vNorm), _mm256_extractf128_ps(vNorm, 1));
+    __m128 vSum128 = _mm_add_ps(_mm256_castps256_ps128(vSum), _mm256_extractf128_ps(vSum, 1));
+
+    // Handle remainder (16 floats) using SSE when totalFloats is not divisible by 32.
+    // This can happen with mono audio where channels=1 and overlapLength % 32 == 16.
+    if (remainderFloats >= 16)
+    {
+        __m128 vTemp, vRef;
+
+        if (IS_ALIGNED)
+        {
+            vTemp = _mm_load_ps(pVec1);
+            vRef  = _mm_load_ps(pVec2);
+            vSum128 = _mm_add_ps(vSum128, _mm_mul_ps(vTemp, vRef));
+            vNorm128 = _mm_add_ps(vNorm128, _mm_mul_ps(vTemp, vTemp));
+
+            vTemp = _mm_load_ps(pVec1 + 4);
+            vRef  = _mm_load_ps(pVec2 + 4);
+            vSum128 = _mm_add_ps(vSum128, _mm_mul_ps(vTemp, vRef));
+            vNorm128 = _mm_add_ps(vNorm128, _mm_mul_ps(vTemp, vTemp));
+
+            vTemp = _mm_load_ps(pVec1 + 8);
+            vRef  = _mm_load_ps(pVec2 + 8);
+            vSum128 = _mm_add_ps(vSum128, _mm_mul_ps(vTemp, vRef));
+            vNorm128 = _mm_add_ps(vNorm128, _mm_mul_ps(vTemp, vTemp));
+
+            vTemp = _mm_load_ps(pVec1 + 12);
+            vRef  = _mm_load_ps(pVec2 + 12);
+            vSum128 = _mm_add_ps(vSum128, _mm_mul_ps(vTemp, vRef));
+            vNorm128 = _mm_add_ps(vNorm128, _mm_mul_ps(vTemp, vTemp));
+        }
+        else
+        {
+            vTemp = _mm_loadu_ps(pVec1);
+            vRef  = _mm_load_ps(pVec2);
+            vSum128 = _mm_add_ps(vSum128, _mm_mul_ps(vTemp, vRef));
+            vNorm128 = _mm_add_ps(vNorm128, _mm_mul_ps(vTemp, vTemp));
+
+            vTemp = _mm_loadu_ps(pVec1 + 4);
+            vRef  = _mm_load_ps(pVec2 + 4);
+            vSum128 = _mm_add_ps(vSum128, _mm_mul_ps(vTemp, vRef));
+            vNorm128 = _mm_add_ps(vNorm128, _mm_mul_ps(vTemp, vTemp));
+
+            vTemp = _mm_loadu_ps(pVec1 + 8);
+            vRef  = _mm_load_ps(pVec2 + 8);
+            vSum128 = _mm_add_ps(vSum128, _mm_mul_ps(vTemp, vRef));
+            vNorm128 = _mm_add_ps(vNorm128, _mm_mul_ps(vTemp, vTemp));
+
+            vTemp = _mm_loadu_ps(pVec1 + 12);
+            vRef  = _mm_load_ps(pVec2 + 12);
+            vSum128 = _mm_add_ps(vSum128, _mm_mul_ps(vTemp, vRef));
+            vNorm128 = _mm_add_ps(vNorm128, _mm_mul_ps(vTemp, vTemp));
+        }
+    }
+
+    #undef IS_ALIGNED
+
+    // Final horizontal sum: reduce 128-bit to scalar
+    __m128 shuf = _mm_shuffle_ps(vNorm128, vNorm128, _MM_SHUFFLE(2, 3, 0, 1));
+    __m128 sums = _mm_add_ps(vNorm128, shuf);
+    shuf = _mm_movehl_ps(shuf, sums);
+    sums = _mm_add_ss(sums, shuf);
+    float norm = _mm_cvtss_f32(sums);
+    anorm = norm;
+
+    shuf = _mm_shuffle_ps(vSum128, vSum128, _MM_SHUFFLE(2, 3, 0, 1));
+    sums = _mm_add_ps(vSum128, shuf);
+    shuf = _mm_movehl_ps(shuf, sums);
+    sums = _mm_add_ss(sums, shuf);
+    float sum = _mm_cvtss_f32(sums);
+
+    return (double)sum / sqrt(norm < 1e-9 ? 1.0 : norm);
+}
+
+
+double TDStretchAVX::calcCrossCorrAccumulate(const float *pV1, const float *pV2, double &norm)
+{
+    return calcCrossCorr(pV1, pV2, norm);
+}
+
+#endif // defined(SOUNDTOUCH_ALLOW_AVX) && (SOUNDTOUCH_ALLOW_AVX + 0) == 1
diff --git a/source/SoundTouch/cpu_detect.h b/source/SoundTouch/cpu_detect.h
index 0794c44..6f89909 100644
--- a/source/SoundTouch/cpu_detect.h
+++ b/source/SoundTouch/cpu_detect.h
@@ -43,6 +43,8 @@
 #define SUPPORT_ALTIVEC     0x0004
 #define SUPPORT_SSE         0x0008
 #define SUPPORT_SSE2        0x0010
+#define SUPPORT_AVX         0x0020
+#define SUPPORT_FMA         0x0040
 
 /// Checks which instruction set extensions are supported by the CPU.
 ///
diff --git a/source/SoundTouch/cpu_detect_x86.cpp b/source/SoundTouch/cpu_detect_x86.cpp
index 18d88e9..721bf12 100644
--- a/source/SoundTouch/cpu_detect_x86.cpp
+++ b/source/SoundTouch/cpu_detect_x86.cpp
@@ -38,17 +38,21 @@
 
 #if defined(SOUNDTOUCH_ALLOW_X86_OPTIMIZATIONS)
 
-   #if defined(__GNUC__) && defined(__i386__)
+   #if defined(__GNUC__) && (defined(__i386__) || defined(__x86_64__))
        // gcc
-       #include "cpuid.h"
-   #elif defined(_M_IX86)
+       #include <cpuid.h>
+   #elif defined(_M_IX86) || defined(_M_X64)
        // windows non-gcc
        #include <intrin.h>
    #endif
 
-   #define bit_MMX     (1 << 23)
-   #define bit_SSE     (1 << 25)
-   #define bit_SSE2    (1 << 26)
+   // Underscores to avoid clashing with cpuid.h defines.
+   #define bit_MMX_     (1 << 23)
+   #define bit_SSE_     (1 << 25)
+   #define bit_SSE2_    (1 << 26)
+   #define bit_OSXSAVE_ (1 << 27)
+   #define bit_AVX_     (1 << 28)
+   #define bit_FMA_     (1 << 12)
 #endif
 
 
@@ -71,21 +75,24 @@ void disableExtensions(uint dwDisableMask)
 /// Checks which instruction set extensions are supported by the CPU.
 uint detectCPUextensions(void)
 {
-/// If building for a 64bit system (no Itanium) and the user wants optimizations.
-/// Return the OR of SUPPORT_{MMX,SSE,SSE2}. 11001 or 0x19.
+/// If building for an x86 or x86-64 system (no Itanium) and the user wants optimizations.
+/// Needs the full check regardless of architecture, since AVX support requires checking for XSAVE/XRSTOR as well.
 /// Keep the _dwDisabledISA test (2 more operations, could be eliminated).
-#if ((defined(__GNUC__) && defined(__x86_64__)) \
-    || defined(_M_X64))  \
-    && defined(SOUNDTOUCH_ALLOW_X86_OPTIMIZATIONS)
-    return 0x19 & ~_dwDisabledISA;
+#if defined(SOUNDTOUCH_ALLOW_X86_OPTIMIZATIONS) && \
+    ((defined(__GNUC__) && (defined(__i386__) || defined(__x86_64__))) || \
+     defined(_M_IX86) || defined(_M_X64))
+
+    if (_dwDisabledISA == 0xffffffff) return 0;
 
-/// If building for a 32bit system and the user wants optimizations.
+/// If building for an x86-64 system and AVX is explicitly disabled,
+/// return the OR of SUPPORT_{MMX,SSE,SSE2}. 11001 or 0x19.
 /// Keep the _dwDisabledISA test (2 more operations, could be eliminated).
-#elif ((defined(__GNUC__) && defined(__i386__)) \
-    || defined(_M_IX86))  \
-    && defined(SOUNDTOUCH_ALLOW_X86_OPTIMIZATIONS)
+#if (defined(SOUNDTOUCH_ALLOW_AVX) && (SOUNDTOUCH_ALLOW_AVX + 0) == 0) && \
+    ((defined(__GNUC__) && defined(__x86_64__)) || defined(_M_X64))
 
-    if (_dwDisabledISA == 0xffffffff) return 0;
+    return 0x19 & ~_dwDisabledISA;
+
+#else
 
     uint res = 0;
 
@@ -94,30 +101,53 @@ uint detectCPUextensions(void)
     uint eax, ebx, ecx, edx;  // unsigned int is the standard type. uint is defined by the compiler and not guaranteed to be portable.
 
     // Check if no cpuid support.
-    if (!__get_cpuid (1, &eax, &ebx, &ecx, &edx)) return 0; // always disable extensions.
-
-    if (edx & bit_MMX)  res = res | SUPPORT_MMX;
-    if (edx & bit_SSE)  res = res | SUPPORT_SSE;
-    if (edx & bit_SSE2) res = res | SUPPORT_SSE2;
-
+    if (!__get_cpuid(1, &eax, &ebx, &ecx, &edx)) return 0; // always disable extensions.
+
+    if (edx & bit_MMX_)  res |= SUPPORT_MMX;
+    if (edx & bit_SSE_)  res |= SUPPORT_SSE;
+    if (edx & bit_SSE2_) res |= SUPPORT_SSE2;
+
+    if ((ecx & bit_OSXSAVE_) && (ecx & bit_AVX_))
+    {
+        uint xcr0_lo, xcr0_hi;
+        __asm__ __volatile__("xgetbv" : "=a"(xcr0_lo), "=d"(xcr0_hi) : "c"(0));
+        if ((xcr0_lo & 0x6) == 0x6)
+        {
+            res |= SUPPORT_AVX;
+            if (ecx & bit_FMA_)
+                res |= SUPPORT_FMA;
+        }
+    }
 #else
     // Window / VS version of cpuid. Notice that Visual Studio 2005 or later required
     // for __cpuid intrinsic support.
     int reg[4] = {-1};
 
     // Check if no cpuid support.
-    __cpuid(reg,0);
+    __cpuid(reg, 0);
     if ((unsigned int)reg[0] == 0) return 0; // always disable extensions.
 
-    __cpuid(reg,1);
-    if ((unsigned int)reg[3] & bit_MMX)  res = res | SUPPORT_MMX;
-    if ((unsigned int)reg[3] & bit_SSE)  res = res | SUPPORT_SSE;
-    if ((unsigned int)reg[3] & bit_SSE2) res = res | SUPPORT_SSE2;
-
+    __cpuid(reg, 1);
+    if ((unsigned int)reg[3] & bit_MMX_)  res |= SUPPORT_MMX;
+    if ((unsigned int)reg[3] & bit_SSE_)  res |= SUPPORT_SSE;
+    if ((unsigned int)reg[3] & bit_SSE2_) res |= SUPPORT_SSE2;
+
+    if (((unsigned int)reg[2] & bit_OSXSAVE_) && ((unsigned int)reg[2] & bit_AVX_))
+    {
+        unsigned long long xcr0 = _xgetbv(0);
+        if ((xcr0 & 0x6) == 0x6)
+        {
+            res |= SUPPORT_AVX;
+            if ((unsigned int)reg[2] & bit_FMA_)
+                res |= SUPPORT_FMA;
+        }
+    }
 #endif
 
     return res & ~_dwDisabledISA;
 
+#endif // defined(SOUNDTOUCH_ALLOW_AVX) && SOUNDTOUCH_ALLOW_AVX == 0
+
 #else
 
 /// One of these is true:
@@ -128,3 +158,4 @@ uint detectCPUextensions(void)
 
 #endif
 }
+
diff --git a/source/SoundTouch/sse_optimized.cpp b/source/SoundTouch/sse_optimized.cpp
index f3511bc..4a48d53 100644
--- a/source/SoundTouch/sse_optimized.cpp
+++ b/source/SoundTouch/sse_optimized.cpp
@@ -68,7 +68,7 @@ double TDStretchSSE::calcCrossCorr(const float *pV1, const float *pV2, double &a
 {
     int i;
     const float *pVec1;
-    const __m128 *pVec2;
+    const float *pVec2;
     __m128 vSum, vNorm;
 
     // Note. It means a major slow-down if the routine needs to tolerate
@@ -83,62 +83,107 @@ double TDStretchSSE::calcCrossCorr(const float *pV1, const float *pV2, double &a
 #ifdef ST_SIMD_AVOID_UNALIGNED
     // Little cheating allowed, return valid correlation only for
     // aligned locations, meaning every second round for stereo sound.
-
-    #define _MM_LOAD    _mm_load_ps
-
     if (((ulongptr)pV1) & 15) return -1e50;    // skip unaligned locations
-
+    #define IS_ALIGNED true
 #else
-    // No cheating allowed, use unaligned load & take the resulting
-    // performance hit.
-    #define _MM_LOAD    _mm_loadu_ps
+    // Check alignment at runtime for optimal load instruction selection
+    const bool isAligned = (((ulongptr)pV1) & 15) == 0;
+    #define IS_ALIGNED isAligned
 #endif
 
     // ensure overlapLength is divisible by 8
     assert((overlapLength % 8) == 0);
 
     // Calculates the cross-correlation value between 'pV1' and 'pV2' vectors
-    // Note: pV2 _must_ be aligned to 16-bit boundary, pV1 need not.
-    pVec1 = (const float*)pV1;
-    pVec2 = (const __m128*)pV2;
+    // Note: pV2 _must_ be aligned to 16-byte boundary, pV1 need not.
+    pVec1 = pV1;
+    pVec2 = pV2;
     vSum = vNorm = _mm_setzero_ps();
 
     // Unroll the loop by factor of 4 * 4 operations. Use same routine for
     // stereo & mono, for mono it just means twice the amount of unrolling.
-    for (i = 0; i < channels * overlapLength / 16; i ++)
+    if (IS_ALIGNED)
+    {
+        // Fast path: both pointers aligned, use _mm_load_ps throughout
+        for (i = 0; i < channels * overlapLength / 16; i++)
+        {
+            __m128 vTemp, vRef;
+
+            vTemp = _mm_load_ps(pVec1);
+            vRef  = _mm_load_ps(pVec2);
+            vSum  = _mm_add_ps(vSum,  _mm_mul_ps(vTemp, vRef));
+            vNorm = _mm_add_ps(vNorm, _mm_mul_ps(vTemp, vTemp));
+
+            vTemp = _mm_load_ps(pVec1 + 4);
+            vRef  = _mm_load_ps(pVec2 + 4);
+            vSum  = _mm_add_ps(vSum, _mm_mul_ps(vTemp, vRef));
+            vNorm = _mm_add_ps(vNorm, _mm_mul_ps(vTemp, vTemp));
+
+            vTemp = _mm_load_ps(pVec1 + 8);
+            vRef  = _mm_load_ps(pVec2 + 8);
+            vSum  = _mm_add_ps(vSum, _mm_mul_ps(vTemp, vRef));
+            vNorm = _mm_add_ps(vNorm, _mm_mul_ps(vTemp, vTemp));
+
+            vTemp = _mm_load_ps(pVec1 + 12);
+            vRef  = _mm_load_ps(pVec2 + 12);
+            vSum  = _mm_add_ps(vSum, _mm_mul_ps(vTemp, vRef));
+            vNorm = _mm_add_ps(vNorm, _mm_mul_ps(vTemp, vTemp));
+
+            pVec1 += 16;
+            pVec2 += 16;
+        }
+    }
+    else
     {
-        __m128 vTemp;
-        // vSum += pV1[0..3] * pV2[0..3]
-        vTemp = _MM_LOAD(pVec1);
-        vSum  = _mm_add_ps(vSum,  _mm_mul_ps(vTemp ,pVec2[0]));
-        vNorm = _mm_add_ps(vNorm, _mm_mul_ps(vTemp ,vTemp));
-
-        // vSum += pV1[4..7] * pV2[4..7]
-        vTemp = _MM_LOAD(pVec1 + 4);
-        vSum  = _mm_add_ps(vSum, _mm_mul_ps(vTemp, pVec2[1]));
-        vNorm = _mm_add_ps(vNorm, _mm_mul_ps(vTemp ,vTemp));
-
-        // vSum += pV1[8..11] * pV2[8..11]
-        vTemp = _MM_LOAD(pVec1 + 8);
-        vSum  = _mm_add_ps(vSum, _mm_mul_ps(vTemp, pVec2[2]));
-        vNorm = _mm_add_ps(vNorm, _mm_mul_ps(vTemp ,vTemp));
-
-        // vSum += pV1[12..15] * pV2[12..15]
-        vTemp = _MM_LOAD(pVec1 + 12);
-        vSum  = _mm_add_ps(vSum, _mm_mul_ps(vTemp, pVec2[3]));
-        vNorm = _mm_add_ps(vNorm, _mm_mul_ps(vTemp ,vTemp));
-
-        pVec1 += 16;
-        pVec2 += 4;
+        // Unaligned pVec1 path, use unaligned loads.
+
+        for (i = 0; i < channels * overlapLength / 16; i++)
+        {
+            __m128 vTemp, vRef;
+
+            vTemp = _mm_loadu_ps(pVec1);
+            vRef  = _mm_load_ps(pVec2);
+            vSum  = _mm_add_ps(vSum,  _mm_mul_ps(vTemp, vRef));
+            vNorm = _mm_add_ps(vNorm, _mm_mul_ps(vTemp, vTemp));
+
+            vTemp = _mm_loadu_ps(pVec1 + 4);
+            vRef  = _mm_load_ps(pVec2 + 4);
+            vSum  = _mm_add_ps(vSum, _mm_mul_ps(vTemp, vRef));
+            vNorm = _mm_add_ps(vNorm, _mm_mul_ps(vTemp, vTemp));
+
+            vTemp = _mm_loadu_ps(pVec1 + 8);
+            vRef  = _mm_load_ps(pVec2 + 8);
+            vSum  = _mm_add_ps(vSum, _mm_mul_ps(vTemp, vRef));
+            vNorm = _mm_add_ps(vNorm, _mm_mul_ps(vTemp, vTemp));
+
+            vTemp = _mm_loadu_ps(pVec1 + 12);
+            vRef  = _mm_load_ps(pVec2 + 12);
+            vSum  = _mm_add_ps(vSum, _mm_mul_ps(vTemp, vRef));
+            vNorm = _mm_add_ps(vNorm, _mm_mul_ps(vTemp, vTemp));
+
+            pVec1 += 16;
+            pVec2 += 16;
+        }
     }
 
-    // return value = vSum[0] + vSum[1] + vSum[2] + vSum[3]
-    float *pvNorm = (float*)&vNorm;
-    float norm = (pvNorm[0] + pvNorm[1] + pvNorm[2] + pvNorm[3]);
+    // Horizontal sum using SSE shuffle+add pattern.
+    // This avoids type punning through pointer casts which is undefined behavior.
+    // vNorm = [a, b, c, d] -> compute a+b+c+d
+    __m128 shuf = _mm_shuffle_ps(vNorm, vNorm, _MM_SHUFFLE(2, 3, 0, 1));
+    __m128 sums = _mm_add_ps(vNorm, shuf);
+    shuf = _mm_movehl_ps(shuf, sums);
+    sums = _mm_add_ss(sums, shuf);
+    float norm = _mm_cvtss_f32(sums);
     anorm = norm;
 
-    float *pvSum = (float*)&vSum;
-    return (double)(pvSum[0] + pvSum[1] + pvSum[2] + pvSum[3]) / sqrt(norm < 1e-9 ? 1.0 : norm);
+    // Same horizontal sum for vSum
+    shuf = _mm_shuffle_ps(vSum, vSum, _MM_SHUFFLE(2, 3, 0, 1));
+    sums = _mm_add_ps(vSum, shuf);
+    shuf = _mm_movehl_ps(shuf, sums);
+    sums = _mm_add_ss(sums, shuf);
+    float sum = _mm_cvtss_f32(sums);
+
+    return (double)sum / sqrt(norm < 1e-9 ? 1.0 : norm);
 
     /* This is approximately corresponding routine in C-language yet without normalization:
     double corr, norm;
diff --git a/source/SoundTouchDLL/Makefile.am b/source/SoundTouchDLL/Makefile.am
index 38b82bc..91d179e 100644
--- a/source/SoundTouchDLL/Makefile.am
+++ b/source/SoundTouchDLL/Makefile.am
@@ -29,7 +29,8 @@ libSoundTouchDll_la_SOURCES=../SoundTouch/AAFilter.cpp ../SoundTouch/FIRFilter.c
     ../SoundTouch/FIFOSampleBuffer.cpp ../SoundTouch/RateTransposer.cpp ../SoundTouch/SoundTouch.cpp \
     ../SoundTouch/TDStretch.cpp ../SoundTouch/sse_optimized.cpp ../SoundTouch/cpu_detect_x86.cpp \
     ../SoundTouch/BPMDetect.cpp ../SoundTouch/PeakFinder.cpp ../SoundTouch/InterpolateLinear.cpp \
-    ../SoundTouch/InterpolateCubic.cpp ../SoundTouch/InterpolateShannon.cpp SoundTouchDLL.cpp
+    ../SoundTouch/InterpolateCubic.cpp ../SoundTouch/InterpolateShannon.cpp ../SoundTouch/avx_optimized.cpp \
+    SoundTouchDLL.cpp
 
 # Compiler flags
 
@@ -37,7 +38,7 @@ libSoundTouchDll_la_SOURCES=../SoundTouch/AAFilter.cpp ../SoundTouch/FIRFilter.c
 AM_LDFLAGS=$(LDFLAGS) -version-info @LIB_SONAME@
 
 if X86
-CXXFLAGS1=-mstackrealign -msse
+CXXFLAGS1=-mstackrealign -msse -mavx
 endif
 
 if X86_64
-- 
2.52.0

From 561f3861fcb40e4b02ef1f6fcdb736ce048c9b04 Mon Sep 17 00:00:00 2001
From: William Horvath <william@horvath.blog>
Date: Fri, 16 Jan 2026 07:17:25 -0800
Subject: [PATCH] Add an AVX FIRFilter implementation.

---
 source/SoundTouch/AAFilter.cpp      |   7 +-
 source/SoundTouch/FIRFilter.cpp     |  46 ++++---
 source/SoundTouch/FIRFilter.h       |  22 +++-
 source/SoundTouch/avx_optimized.cpp | 196 ++++++++++++++++++++++++++++
 4 files changed, 254 insertions(+), 17 deletions(-)

diff --git a/source/SoundTouch/AAFilter.cpp b/source/SoundTouch/AAFilter.cpp
index f097200..fc44f0a 100644
--- a/source/SoundTouch/AAFilter.cpp
+++ b/source/SoundTouch/AAFilter.cpp
@@ -39,6 +39,7 @@
 #include <stdlib.h>
 #include "AAFilter.h"
 #include "FIRFilter.h"
+#include "STTypes.h"
 
 using namespace soundtouch;
 
@@ -101,7 +102,11 @@ void AAFilter::setCutoffFreq(double newCutoffFreq)
 // Sets number of FIR filter taps
 void AAFilter::setLength(uint newLength)
 {
-    length = newLength;
+    // Make sure it's divisible by the SIMD alignment
+    if (newLength == 0) {
+        newLength = SOUNDTOUCH_OPTIMAL_DIVISIBILITY;
+    }
+    length = ( ((uint)(newLength) + (uint)(SOUNDTOUCH_OPTIMAL_DIVISIBILITY - 1) ) & ~(uint)(SOUNDTOUCH_OPTIMAL_DIVISIBILITY - 1));
     calculateCoeffs();
 }
 
diff --git a/source/SoundTouch/FIRFilter.cpp b/source/SoundTouch/FIRFilter.cpp
index 3b516dc..8183a8a 100644
--- a/source/SoundTouch/FIRFilter.cpp
+++ b/source/SoundTouch/FIRFilter.cpp
@@ -43,6 +43,7 @@
 #include <math.h>
 #include <stdlib.h>
 #include "FIRFilter.h"
+#include "STTypes.h"
 #include "cpu_detect.h"
 
 using namespace soundtouch;
@@ -74,8 +75,9 @@ FIRFilter::~FIRFilter()
 uint FIRFilter::evaluateFilterStereo(SAMPLETYPE *dest, const SAMPLETYPE *src, uint numSamples) const
 {
     int j, end;
-    // hint compiler autovectorization that loop length is divisible by 8
-    uint ilength = length & -8;
+
+    // hint compiler autovectorization that loop length is divisible by 8/16
+    uint ilength = length & -SOUNDTOUCH_OPTIMAL_DIVISIBILITY;
 
     assert((length != 0) && (length == ilength) && (src != nullptr) && (dest != nullptr) && (filterCoeffs != nullptr));
     assert(numSamples >= ilength);
@@ -117,8 +119,8 @@ uint FIRFilter::evaluateFilterMono(SAMPLETYPE *dest, const SAMPLETYPE *src, uint
 {
     int j, end;
 
-    // hint compiler autovectorization that loop length is divisible by 8
-    int ilength = length & -8;
+    // hint compiler autovectorization that loop length is divisible by 8/16
+    int ilength = length & -SOUNDTOUCH_OPTIMAL_DIVISIBILITY;
 
     assert(ilength != 0);
 
@@ -156,8 +158,8 @@ uint FIRFilter::evaluateFilterMulti(SAMPLETYPE *dest, const SAMPLETYPE *src, uin
     assert(filterCoeffs != nullptr);
     assert(numChannels <= SOUNDTOUCH_MAX_CHANNELS);
 
-    // hint compiler autovectorization that loop length is divisible by 8
-    int ilength = length & -8;
+    // hint compiler autovectorization that loop length is divisible by 8/16
+    int ilength = length & -SOUNDTOUCH_OPTIMAL_DIVISIBILITY;
 
     end = numChannels * (numSamples - ilength);
 
@@ -200,15 +202,20 @@ uint FIRFilter::evaluateFilterMulti(SAMPLETYPE *dest, const SAMPLETYPE *src, uin
 
 // Set filter coeffiecients and length.
 //
-// Throws an exception if filter length isn't divisible by 8
+// Throws an exception if filter length isn't divisible by 8/16
 void FIRFilter::setCoefficients(const SAMPLETYPE *coeffs, uint newLength, uint uResultDivFactor)
 {
     assert(newLength > 0);
-    if (newLength % 8) ST_THROW_RT_ERROR("FIR filter length not divisible by 8");
 
+#define ST_QUOTE(x) #x
+#define ST_STRING(x) ST_QUOTE(x)
+    if (newLength % SOUNDTOUCH_OPTIMAL_DIVISIBILITY) ST_THROW_RT_ERROR("FIR filter length not divisible by " ST_STRING(SOUNDTOUCH_OPTIMAL_DIVISIBILITY) );
+#undef ST_STRING
+#undef ST_QUOTE
+
+    length = newLength;
     lengthDiv8 = newLength / 8;
-    length = lengthDiv8 * 8;
-    assert(length == newLength);
+    assert(lengthDiv8 * 8 == newLength);
 
     resultDivFactor = uResultDivFactor;
 
@@ -289,14 +296,14 @@ FIRFilter * FIRFilter::newInstance()
 
     // Check if MMX/SSE instruction set extensions supported by CPU
 
-#ifdef SOUNDTOUCH_ALLOW_MMX
-    // MMX routines available only with integer sample types
-    if (uExtensions & SUPPORT_MMX)
+#if defined(SOUNDTOUCH_ALLOW_AVX) && (SOUNDTOUCH_ALLOW_AVX + 0) == 1
+    if (uExtensions & SUPPORT_AVX)
     {
-        return ::new FIRFilterMMX;
+        // AVX support
+        return ::new FIRFilterAVX(uExtensions & SUPPORT_FMA);
     }
     else
-#endif // SOUNDTOUCH_ALLOW_MMX
+#endif // defined(SOUNDTOUCH_ALLOW_AVX) && (SOUNDTOUCH_ALLOW_AVX + 0) == 1
 
 #ifdef SOUNDTOUCH_ALLOW_SSE
     if (uExtensions & SUPPORT_SSE)
@@ -307,6 +314,15 @@ FIRFilter * FIRFilter::newInstance()
     else
 #endif // SOUNDTOUCH_ALLOW_SSE
 
+#ifdef SOUNDTOUCH_ALLOW_MMX
+    // MMX routines available only with integer sample types
+    if (uExtensions & SUPPORT_MMX)
+    {
+        return ::new FIRFilterMMX;
+    }
+    else
+#endif // SOUNDTOUCH_ALLOW_MMX
+
     {
         // ISA optimizations not supported, use plain C version
         return ::new FIRFilter;
diff --git a/source/SoundTouch/FIRFilter.h b/source/SoundTouch/FIRFilter.h
index 0ae5083..1336633 100644
--- a/source/SoundTouch/FIRFilter.h
+++ b/source/SoundTouch/FIRFilter.h
@@ -46,7 +46,7 @@ class FIRFilter
 protected:
     // Number of FIR filter taps
     uint length;
-    // Number of FIR filter taps divided by 8
+    // Number of FIR filter taps divided by 8 (only used by MMX)
     uint lengthDiv8;
 
     // Result divider factor in 2^k format
@@ -132,6 +132,26 @@ public:
 
 #endif // SOUNDTOUCH_ALLOW_SSE
 
+#if defined(SOUNDTOUCH_ALLOW_AVX) && (SOUNDTOUCH_ALLOW_AVX + 0) == 1
+    /// Class that implements AVX optimized functions exclusive for floating point samples type.
+    class FIRFilterAVX : public FIRFilter
+    {
+    private:
+        bool bSupportsFMA;
+    protected:
+        float *filterCoeffsUnalign;
+        float *filterCoeffsAlign;
+
+        virtual uint evaluateFilterStereo(float *dest, const float *src, uint numSamples) const override;
+    public:
+        FIRFilterAVX(bool useFMA);
+        ~FIRFilterAVX();
+
+        virtual void setCoefficients(const float *coeffs, uint newLength, uint uResultDivFactor) override;
+    };
+
+#endif // defined(SOUNDTOUCH_ALLOW_AVX) && (SOUNDTOUCH_ALLOW_AVX + 0) == 1
+
 }
 
 #endif  // FIRFilter_H
diff --git a/source/SoundTouch/avx_optimized.cpp b/source/SoundTouch/avx_optimized.cpp
index d7496de..179ada5 100644
--- a/source/SoundTouch/avx_optimized.cpp
+++ b/source/SoundTouch/avx_optimized.cpp
@@ -301,4 +301,200 @@ double TDStretchAVX::calcCrossCorrAccumulate(const float *pV1, const float *pV2,
     return calcCrossCorr(pV1, pV2, norm);
 }
 
+//////////////////////////////////////////////////////////////////////////////
+//
+// implementation of AVX optimized functions of class 'FIRFilterAVX'
+//
+//////////////////////////////////////////////////////////////////////////////
+
+#include "FIRFilter.h"
+
+FIRFilterAVX::FIRFilterAVX(bool useFMA) : FIRFilter()
+{
+    bSupportsFMA = useFMA;
+    filterCoeffsAlign = nullptr;
+    filterCoeffsUnalign = nullptr;
+}
+
+
+FIRFilterAVX::~FIRFilterAVX()
+{
+    delete[] filterCoeffsUnalign;
+    filterCoeffsAlign = nullptr;
+    filterCoeffsUnalign = nullptr;
+}
+
+
+void FIRFilterAVX::setCoefficients(const float *coeffs, uint newLength, uint uResultDivFactor)
+{
+    FIRFilter::setCoefficients(coeffs, newLength, uResultDivFactor);
+
+    delete[] filterCoeffsUnalign;
+    filterCoeffsUnalign = new float[2 * newLength + 8];
+    filterCoeffsAlign = (float *)SOUNDTOUCH_ALIGN_POINTER(filterCoeffsUnalign);
+
+    const float scale = (float)::pow(0.5, (int)resultDivFactor);
+
+    for (uint i = 0; i < newLength; i++)
+    {
+        filterCoeffsAlign[2 * i + 0] =
+        filterCoeffsAlign[2 * i + 1] = coeffs[i] * scale;
+    }
+}
+
+
+// Horizontal reduce __m256 to stereo sample and store
+static inline void reduceAndStore2(__m128 &out, __m256 sum0, __m256 sum1)
+{
+    __m128 s0 = _mm_add_ps(_mm256_castps256_ps128(sum0), _mm256_extractf128_ps(sum0, 1));
+    __m128 s1 = _mm_add_ps(_mm256_castps256_ps128(sum1), _mm256_extractf128_ps(sum1, 1));
+    s0 = _mm_add_ps(s0, _mm_shuffle_ps(s0, s0, _MM_SHUFFLE(1, 0, 3, 2)));
+    s1 = _mm_add_ps(s1, _mm_shuffle_ps(s1, s1, _MM_SHUFFLE(1, 0, 3, 2)));
+    out = _mm_shuffle_ps(s0, s1, _MM_SHUFFLE(1, 0, 1, 0));
+}
+
+
+uint FIRFilterAVX::evaluateFilterStereo(float *dest, const float *source, uint numSamples) const
+{
+    assert(source != nullptr);
+    assert(dest != nullptr);
+    assert((length % 8) == 0);
+    assert(filterCoeffsAlign != nullptr);
+    assert(((ulongptr)filterCoeffsAlign) % 32 == 0);
+
+    const int count = (int)(numSamples - length);
+    if (count < 8) return 0;
+
+    // Process 8 output stereo samples per iteration to maximize coefficient sharing
+    const int count8 = count & ~7;
+    int j;
+
+    #pragma omp parallel for
+    for (j = 0; j < count8; j += 8)
+    {
+        const float *pSrc = source + j * 2;
+        const float *pFil = filterCoeffsAlign;
+
+        __m256 sum0 = _mm256_setzero_ps();
+        __m256 sum1 = _mm256_setzero_ps();
+        __m256 sum2 = _mm256_setzero_ps();
+        __m256 sum3 = _mm256_setzero_ps();
+        __m256 sum4 = _mm256_setzero_ps();
+        __m256 sum5 = _mm256_setzero_ps();
+        __m256 sum6 = _mm256_setzero_ps();
+        __m256 sum7 = _mm256_setzero_ps();
+
+        if (bSupportsFMA)
+        {
+            for (uint i = 0; i < length / 8; i++)
+            {
+                __m256 c0 = _mm256_load_ps(pFil);
+                __m256 c1 = _mm256_load_ps(pFil + 8);
+
+                sum0 = _mm256_fmadd_ps(_mm256_loadu_ps(pSrc), c0, sum0);
+                sum1 = _mm256_fmadd_ps(_mm256_loadu_ps(pSrc + 2), c0, sum1);
+                sum2 = _mm256_fmadd_ps(_mm256_loadu_ps(pSrc + 4), c0, sum2);
+                sum3 = _mm256_fmadd_ps(_mm256_loadu_ps(pSrc + 6), c0, sum3);
+                sum4 = _mm256_fmadd_ps(_mm256_loadu_ps(pSrc + 8), c0, sum4);
+                sum5 = _mm256_fmadd_ps(_mm256_loadu_ps(pSrc + 10), c0, sum5);
+                sum6 = _mm256_fmadd_ps(_mm256_loadu_ps(pSrc + 12), c0, sum6);
+                sum7 = _mm256_fmadd_ps(_mm256_loadu_ps(pSrc + 14), c0, sum7);
+
+                sum0 = _mm256_fmadd_ps(_mm256_loadu_ps(pSrc + 8), c1, sum0);
+                sum1 = _mm256_fmadd_ps(_mm256_loadu_ps(pSrc + 10), c1, sum1);
+                sum2 = _mm256_fmadd_ps(_mm256_loadu_ps(pSrc + 12), c1, sum2);
+                sum3 = _mm256_fmadd_ps(_mm256_loadu_ps(pSrc + 14), c1, sum3);
+                sum4 = _mm256_fmadd_ps(_mm256_loadu_ps(pSrc + 16), c1, sum4);
+                sum5 = _mm256_fmadd_ps(_mm256_loadu_ps(pSrc + 18), c1, sum5);
+                sum6 = _mm256_fmadd_ps(_mm256_loadu_ps(pSrc + 20), c1, sum6);
+                sum7 = _mm256_fmadd_ps(_mm256_loadu_ps(pSrc + 22), c1, sum7);
+
+                pSrc += 16;
+                pFil += 16;
+            }
+        }
+        else
+        {
+            for (uint i = 0; i < length / 8; i++)
+            {
+                __m256 c0 = _mm256_load_ps(pFil);
+                __m256 c1 = _mm256_load_ps(pFil + 8);
+
+                sum0 = _mm256_add_ps(sum0, _mm256_mul_ps(_mm256_loadu_ps(pSrc), c0));
+                sum1 = _mm256_add_ps(sum1, _mm256_mul_ps(_mm256_loadu_ps(pSrc + 2), c0));
+                sum2 = _mm256_add_ps(sum2, _mm256_mul_ps(_mm256_loadu_ps(pSrc + 4), c0));
+                sum3 = _mm256_add_ps(sum3, _mm256_mul_ps(_mm256_loadu_ps(pSrc + 6), c0));
+                sum4 = _mm256_add_ps(sum4, _mm256_mul_ps(_mm256_loadu_ps(pSrc + 8), c0));
+                sum5 = _mm256_add_ps(sum5, _mm256_mul_ps(_mm256_loadu_ps(pSrc + 10), c0));
+                sum6 = _mm256_add_ps(sum6, _mm256_mul_ps(_mm256_loadu_ps(pSrc + 12), c0));
+                sum7 = _mm256_add_ps(sum7, _mm256_mul_ps(_mm256_loadu_ps(pSrc + 14), c0));
+
+                sum0 = _mm256_add_ps(sum0, _mm256_mul_ps(_mm256_loadu_ps(pSrc + 8), c1));
+                sum1 = _mm256_add_ps(sum1, _mm256_mul_ps(_mm256_loadu_ps(pSrc + 10), c1));
+                sum2 = _mm256_add_ps(sum2, _mm256_mul_ps(_mm256_loadu_ps(pSrc + 12), c1));
+                sum3 = _mm256_add_ps(sum3, _mm256_mul_ps(_mm256_loadu_ps(pSrc + 14), c1));
+                sum4 = _mm256_add_ps(sum4, _mm256_mul_ps(_mm256_loadu_ps(pSrc + 16), c1));
+                sum5 = _mm256_add_ps(sum5, _mm256_mul_ps(_mm256_loadu_ps(pSrc + 18), c1));
+                sum6 = _mm256_add_ps(sum6, _mm256_mul_ps(_mm256_loadu_ps(pSrc + 20), c1));
+                sum7 = _mm256_add_ps(sum7, _mm256_mul_ps(_mm256_loadu_ps(pSrc + 22), c1));
+
+                pSrc += 16;
+                pFil += 16;
+            }
+        }
+
+        // Reduce and store 8 stereo output samples
+        float *pDest = dest + j * 2;
+        __m128 out01, out23, out45, out67;
+        reduceAndStore2(out01, sum0, sum1);
+        reduceAndStore2(out23, sum2, sum3);
+        reduceAndStore2(out45, sum4, sum5);
+        reduceAndStore2(out67, sum6, sum7);
+        _mm_storeu_ps(pDest, out01);
+        _mm_storeu_ps(pDest + 4, out23);
+        _mm_storeu_ps(pDest + 8, out45);
+        _mm_storeu_ps(pDest + 12, out67);
+    }
+
+    // Handle remaining samples (up to 7) with simple 2-output loop
+    for (; j < count; j += 2)
+    {
+        const float *pSrc = source + j * 2;
+        const float *pFil = filterCoeffsAlign;
+
+        __m256 sum0 = _mm256_setzero_ps();
+        __m256 sum1 = _mm256_setzero_ps();
+
+        for (uint i = 0; i < length / 8; i++)
+        {
+            __m256 c0 = _mm256_load_ps(pFil);
+            __m256 c1 = _mm256_load_ps(pFil + 8);
+
+            if (bSupportsFMA)
+            {
+                sum0 = _mm256_fmadd_ps(_mm256_loadu_ps(pSrc), c0, sum0);
+                sum1 = _mm256_fmadd_ps(_mm256_loadu_ps(pSrc + 2), c0, sum1);
+                sum0 = _mm256_fmadd_ps(_mm256_loadu_ps(pSrc + 8), c1, sum0);
+                sum1 = _mm256_fmadd_ps(_mm256_loadu_ps(pSrc + 10), c1, sum1);
+            }
+            else
+            {
+                sum0 = _mm256_add_ps(sum0, _mm256_mul_ps(_mm256_loadu_ps(pSrc), c0));
+                sum1 = _mm256_add_ps(sum1, _mm256_mul_ps(_mm256_loadu_ps(pSrc + 2), c0));
+                sum0 = _mm256_add_ps(sum0, _mm256_mul_ps(_mm256_loadu_ps(pSrc + 8), c1));
+                sum1 = _mm256_add_ps(sum1, _mm256_mul_ps(_mm256_loadu_ps(pSrc + 10), c1));
+            }
+
+            pSrc += 16;
+            pFil += 16;
+        }
+
+        __m128 out;
+        reduceAndStore2(out, sum0, sum1);
+        _mm_storeu_ps(dest + j * 2, out);
+    }
+
+    return (uint)count;
+}
+
 #endif // defined(SOUNDTOUCH_ALLOW_AVX) && (SOUNDTOUCH_ALLOW_AVX + 0) == 1
-- 
2.52.0

From c20acc1e6a7217203cc2066a5129d9b4eef58259 Mon Sep 17 00:00:00 2001
From: William Horvath <william@horvath.blog>
Date: Fri, 16 Jan 2026 08:19:28 -0800
Subject: [PATCH] Avoid inlining AVX functions when the build does not support
 them

---
 include/STTypes.h                   | 12 ++++++++++++
 source/SoundTouch/FIRFilter.h       |  2 +-
 source/SoundTouch/TDStretch.h       |  4 ++--
 source/SoundTouch/avx_optimized.cpp |  8 ++++----
 4 files changed, 19 insertions(+), 7 deletions(-)

diff --git a/include/STTypes.h b/include/STTypes.h
index 3e5e4a2..e96e10b 100644
--- a/include/STTypes.h
+++ b/include/STTypes.h
@@ -49,6 +49,11 @@ typedef unsigned long   ulong;
     #include "soundtouch_config.h"
 #endif
 
+#ifdef _MSC_VER
+#define SOUNDTOUCH_COMPILE_TARGET_ATTRIBUTE(...)
+#else
+#define SOUNDTOUCH_COMPILE_TARGET_ATTRIBUTE(...) __attribute__((__target__(__VA_ARGS__)))
+#endif
 
 namespace soundtouch
 {
@@ -158,6 +163,13 @@ namespace soundtouch
                 // Allow AVX optimizations, but also allow
                 // overriding AVX support with -DSOUNDTOUCH_ALLOW_AVX=0.
                 #define SOUNDTOUCH_ALLOW_AVX   1
+                // If not compiling with support directly, force compilation with the correct attributes.
+                // This is not required for MSVC.
+                #if (defined(__AVX__) && defined(__FMA__)) || defined(__AVX2__)
+                    #define SOUNDTOUCH_AVX_ATTRIBUTES
+                #else
+                    #define SOUNDTOUCH_AVX_ATTRIBUTES SOUNDTOUCH_COMPILE_TARGET_ATTRIBUTE("avx,fma")
+                #endif
             #endif
         #endif
 
diff --git a/source/SoundTouch/FIRFilter.h b/source/SoundTouch/FIRFilter.h
index 1336633..3a50506 100644
--- a/source/SoundTouch/FIRFilter.h
+++ b/source/SoundTouch/FIRFilter.h
@@ -142,7 +142,7 @@ public:
         float *filterCoeffsUnalign;
         float *filterCoeffsAlign;
 
-        virtual uint evaluateFilterStereo(float *dest, const float *src, uint numSamples) const override;
+        virtual SOUNDTOUCH_AVX_ATTRIBUTES uint evaluateFilterStereo(float *dest, const float *src, uint numSamples) const override;
     public:
         FIRFilterAVX(bool useFMA);
         ~FIRFilterAVX();
diff --git a/source/SoundTouch/TDStretch.h b/source/SoundTouch/TDStretch.h
index 5a07268..ecaf36b 100644
--- a/source/SoundTouch/TDStretch.h
+++ b/source/SoundTouch/TDStretch.h
@@ -283,8 +283,8 @@ public:
     private:
         bool bSupportsFMA;
     protected:
-        double calcCrossCorr(const float *mixingPos, const float *compare, double &norm) override;
-        double calcCrossCorrAccumulate(const float *mixingPos, const float *compare, double &norm) override;
+        SOUNDTOUCH_AVX_ATTRIBUTES double calcCrossCorr(const float *mixingPos, const float *compare, double &norm) override;
+        SOUNDTOUCH_AVX_ATTRIBUTES double calcCrossCorrAccumulate(const float *mixingPos, const float *compare, double &norm) override;
     public:
         TDStretchAVX(bool supportsFMA);
     };
diff --git a/source/SoundTouch/avx_optimized.cpp b/source/SoundTouch/avx_optimized.cpp
index 179ada5..0b3ded4 100644
--- a/source/SoundTouch/avx_optimized.cpp
+++ b/source/SoundTouch/avx_optimized.cpp
@@ -61,7 +61,7 @@ TDStretchAVX::TDStretchAVX(bool supportsFMA) : TDStretch() {
 }
 
 // Calculates cross correlation of two buffers
-double TDStretchAVX::calcCrossCorr(const float *pV1, const float *pV2, double &anorm)
+SOUNDTOUCH_AVX_ATTRIBUTES double TDStretchAVX::calcCrossCorr(const float *pV1, const float *pV2, double &anorm)
 {
     int i;
     const float *pVec1;
@@ -296,7 +296,7 @@ double TDStretchAVX::calcCrossCorr(const float *pV1, const float *pV2, double &a
 }
 
 
-double TDStretchAVX::calcCrossCorrAccumulate(const float *pV1, const float *pV2, double &norm)
+SOUNDTOUCH_AVX_ATTRIBUTES double TDStretchAVX::calcCrossCorrAccumulate(const float *pV1, const float *pV2, double &norm)
 {
     return calcCrossCorr(pV1, pV2, norm);
 }
@@ -344,7 +344,7 @@ void FIRFilterAVX::setCoefficients(const float *coeffs, uint newLength, uint uRe
 
 
 // Horizontal reduce __m256 to stereo sample and store
-static inline void reduceAndStore2(__m128 &out, __m256 sum0, __m256 sum1)
+static SOUNDTOUCH_AVX_ATTRIBUTES inline void reduceAndStore2(__m128 &out, __m256 sum0, __m256 sum1)
 {
     __m128 s0 = _mm_add_ps(_mm256_castps256_ps128(sum0), _mm256_extractf128_ps(sum0, 1));
     __m128 s1 = _mm_add_ps(_mm256_castps256_ps128(sum1), _mm256_extractf128_ps(sum1, 1));
@@ -354,7 +354,7 @@ static inline void reduceAndStore2(__m128 &out, __m256 sum0, __m256 sum1)
 }
 
 
-uint FIRFilterAVX::evaluateFilterStereo(float *dest, const float *source, uint numSamples) const
+SOUNDTOUCH_AVX_ATTRIBUTES uint FIRFilterAVX::evaluateFilterStereo(float *dest, const float *source, uint numSamples) const
 {
     assert(source != nullptr);
     assert(dest != nullptr);
-- 
2.52.0

